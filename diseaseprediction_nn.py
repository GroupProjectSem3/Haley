# -*- coding: utf-8 -*-
"""updated_haley.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-IiaCKT0VWyeoxQOWJWcYH_ghKwM7AM7
"""

#if you are not using google collab please ignore this
#to mount google drive
from google.colab import drive
drive.mount('/content/drive')

import numpy as np # linear algebra
import pandas as pd

import tensorflow as tf
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD

#df = pd.read_csv("/content/drive/My Drive/Group_Project/Binary-200row.csv")
df = pd.read_csv("/content/drive/My Drive/Group_Project/Binary200shuff.csv")

X = np.array(df.iloc[:, df.columns != 'Disease'])
y = np.array(df.iloc[:, df.columns == 'Disease'])

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
#Normalization
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from keras.models import Sequential
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import ShuffleSplit
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
import tensorflow as tf
from keras.models import *
from keras.layers import *
from keras.callbacks import *
from sklearn.model_selection import  ShuffleSplit, cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix, classification_report

# %matplotlib inline

#train-test split
x_train, x_test, y_train, y_test = train_test_split(X,y,
                                                    test_size=.20,
                                                    random_state=42, stratify = y)

#np.unique([1, 1, 2, 2, 3, 3])
values=np.unique(y_train)
values

Y_train=np.zeros(shape=(len(y_train),len(values)))
k=0
for x in y_train:
     for i in range(len(values)):
            if x==values[i]:
                tmp=list(np.zeros(25))
                tmp[i]=1
                Y_train[k]=tmp
                k+=1

Y_train[1]

model = Sequential()
model.add(Dense(4920, input_dim=100))
model.add(Activation('relu'))
model.add(Dense(2500))
model.add(Activation('relu'))
model.add(Dense(25))
model.add(Activation('sigmoid'))

sgd = SGD(lr=0.1)
model.compile(loss='binary_crossentropy',metrics=['accuracy'], optimizer=sgd)

model.fit(x_train, Y_train, batch_size=1, epochs=3)

model.summary()

#for saving the model
model.save('disease_prediction.h5')  # creates a HDF5 file 'haley_cnn.h5'

#directly use this line in app file and change the path of h5 file accordingly
disease_prediction = tf.keras.models.load_model('/content/disease_prediction.h5')

#np.unique([1, 1, 2, 2, 3, 3])
test_values=np.unique(y_test)
test_values

Y_test=np.zeros(shape=(len(y_test),len(test_values)))
k=0
for x in y_test:
     for i in range(len(test_values)):
            if x==test_values[i]:
                tmp=list(np.zeros(25))
                tmp[i]=1
                Y_test[k]=tmp
                k+=1

indices = [i for i in range(100)]
symptoms = df.columns.values[:-1]

dictionary = dict(zip(symptoms,indices))

def dosomething(symptom):
    user_input_symptoms = symptom
    user_input_label = [0 for i in range(100)]
    for i in user_input_symptoms:
        idx = dictionary[i]
        user_input_label[idx] = 1

    user_input_label = np.array(user_input_label)
    user_input_label = user_input_label.reshape((-1,1)).transpose()
    return(disease_prediction.predict(user_input_label))

ds = dosomething(['Sneezing', 'Sore Throat', 'Stuffy Nose' ])

ds = dosomething(['Fatigue','Diarrhea','Constipation']) #doing something

L = np.argsort(-ds, axis=1)

#for printing 3 values
print(values[L[:,0]])
print(values[L[:,1]])
print(values[L[:,2]])

cc = L[:,1]

cc1 = L[:,1]

# for printing percentage of the occurences
print(ds[0,cc]*100)
print(ds[0,L[:,1]]*100)
print(ds[0,L[:,2]]*100)

pre=model.predict(x_test)

acc=0;
for i in range(len(pre)):
    if pre[i].argmax() == Y_test[i].argmax() :
        if pre[i].max() >= 0.80:
            acc+=1

acc=acc/len(pre)
acc

pre[8].max()

acc=0;
for i in range(len(pre)):
    if pre[i].argmax() == Y_test[i].argmax() :
        if pre[i].max() >= 0.80:
            acc+=1

acc=acc/len(pre)
acc
#below this line not imp.
'''
for i in range(len(pre)):
    pre[i] = pre[i].argmax()
    y_test[i] = y_test[i].argmax()

pre = np.argmax(pre)
y_test = np.argmax(y_test, axis=1)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, pre)*100)

for i in range(len(pre)):
  conf[i] = confusion_matrix(y_test[i],pre[i])
print(conf[1])
'''